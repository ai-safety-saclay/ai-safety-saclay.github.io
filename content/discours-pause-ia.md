---
title: Stop aux IA
---


**BUG**: veuillez rafraîchir la page avant de lire.

[:Stop aux IA (cliquer pour déplier)](#Discourse)

## :x Discourse

Aujourd'hui, [:les experts sont d'accord](#Lavisdesexperts): il faut réglementer fortement l'Intelligence Artificielle (IA) et financer bien plus la recherche dans le domaine de la [:sûreté des IA](#AIsafety), sous risque de [:catastrophe](#Catastrophe). Les systèmes d'IA sont de [:plus en plus capables et autonomes](#Capabilities), et en même temps [:peu fiables](#Fiability) et difficiles à contrôler. On observe déjà des [:dérives](#Misuse) aujourd'hui, et cela ne va faire qu'empirer. Selon les entreprises à la pointe et de nombreux chercheurs, un système meilleur que les humains dans toutes les taches est à prévoir dans [:quelques années, avec tout ce que cela implique](#Timeline). La [:législation Européenne sur l'intelligence artificielle](#EUAIact) doit être adoptée de manière stricte, malgré la pression d'entreprises qui détournent l'attention pour utiliser l'IA pour leurs profits. Heureusement, grâce au travail de chercheurs et de législateurs, la régulation est tout à fait applicable, si on s'en donne les moyens. Mais cela ne suffit pas. Pour réduire les risques de catastrophes, une [:pause du développement des systèmes d'IA avancés](#Pause) est devenue nécessaire, jusqu'à ce que nous puissions garantir leur développement sûr et leur contrôle démocratique. 

Nous sommes [:pause IA](#Pause), nous avons des propositions précises et nous ferons tout ce qui est en notre pouvoir pour les faire accepter.


## :x Timeline

OpenAI, l'entreprise ayant créé chatGPT, prévoit d'automatiser l'entièreté des tâches intellectuelles humaines d'ici 2028. Cela inclut toute forme d'écriture, d'invention, de management, de création de plan à long terme, et même de développement de nouvelles IA.

La plupart des experts estiment que cette date est trop ambitieuse, en revanche ils s'accordent sur le fait qu'une forme d'IA radicalement différente peut être créée [avant 2030](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/).

C'est un point de non retour: un tel système pourrait faire un plan sur plusieurs années, s'améliorer, se répandre sur différents serveurs, manipuler chacun de nous, et même contrôler des appareils connectés.


## :x AI safety

La sûreté des IA est un champ de recherche visant à comprendre et à contrôler les agents [:de plus en plus puissants](#Capabilities) que l'on crée.

Aujourd'hui, les IA sont des boites noires

- dont on ne [:comprend pas bien le fonctionnement](#Understanding)
- qui [:surprennent sans cesse les chercheurs](#Chess)
- auquel il est difficile de spécifier un objectif

Ce sont comme des enfants très doués qui apprennent toujours plus vite, mais qui ne comprennent pas vraiment le monde des adultes, ni leur morale.

## :x Understanding

Les chercheurs utilisant les réseaux de neurones sont comme des biologistes qui font des cultures de bactéries. Ils ont conçu l'expérience et connaissent les conditions initiales, mais cela ne veut pas dire qu'ils comprennent tout ce qui se passe.


## :x Chess

En 2024, on a découvert que le modèle de langage GPT3.5, entrainé à compléter du texte, est capable de jouer aux échecs mieux que la moyenne des joueurs humains.

[source](https://blog.mathieuacher.com/GPTsChessEloRatingLegalMoves/)

## :x L'avis des experts

En 2016, 50% des experts pensaient qu'il faut prioriser nettement plus la recherche en [:sûreté des IA](#AIsafety).

En 2022, c'était 75%.

[![](https://wiki.aiimpacts.org/_media/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/how_much_should_society_prioritize_ai_safety_research_relative_to_how_much_it_is_currently_prioritized_1_.png)](https://wiki.aiimpacts.org/doku.php?id=ai_timelines:predictions_of_human-level_ai_timelines:ai_timeline_surveys:2022_expert_survey_on_progress_in_ai#safety)

Cette proportion continue d'augmenter.


Un tiers des experts considère que les risques liés aux IA sont comparables aux risques de guerre nucléaire mondiale.

Il faut prendre en compte qu'une partie de ces experts travaille pour des géants du numérique, qui bénéficient le plus des progrès en IA. Il y a un conflit d'intéret en défaveur de la sûreté et de la régulation.


## :x EU AI act

En mars 2024, le parlement européen adopte cette proposition de loi à 523 voix contre 46.

Elle a deux missions:
- soumettre à une régulation stricte l'usage des IA dans les domaines critiques (santé, éducation, travail)
- surveiller l'usage général des systèmes d'IA, qui peuvent modifier nos rapports sociaux et l'économie.

## :x Capabilities

Les meilleurs systèmes d'IA sont à la fois plus performants que les humains dans des domaines de la vie de tous les jours, mais également dans des domaines d'experts.

Système le plus puissant à l'heure actuelle, GPT4 peut lire du texte et regarder des images. Il peut répondre à n'importe quelle question sur presque tous les sujets.
Il obtient de meilleurs résultats que l'étudiant moyen, ceci sur la plupart des concours de niveau bac.

[![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B8Djo44WtZK6kK4K5/gs4nviwjtytjzhpidxmh)](https://www.lesswrong.com/posts/B8Djo44WtZK6kK4K5/outreach-success-intro-to-ai-risk-that-has-been-successful#AI_capabilities)


L'IA peut aussi effectuer des tâches où il simplement impossible de rivaliser. En quelques secondes, une IA peut créer des images, générer des vidéos, lire et résumer des dizaines d'articles.

## :x Catastrophe

Le risque de catastrophe créé par l'IA n'est pas un simple délire de Science-Fiction.

C'est un outil qui permet d’accélérer tous les risques de catastrophe actuels. Il pourrait tout aussi bien créer des virus informatiques en continu, permettre la surveillance à grande échelle ou superviser une guerre entière.

Ce qui rend le risque considérable, c'est la capacité qu'ont les IA - déjà aujourd'hui - à coder des logiciels, trouver des failles de sécurité, se répliquer sur d'autres serveurs et prévoir des stratégies à long terme.

## :x Misuse

L'IA est une arme surpuissante.

Elle permet à des agences de [génèrer de la désinformation](https://web.archive.org/web/20230221214839/https://www.haaretz.com/israel-news/security-aviation/2023-02-15/ty-article-magazine/.premium/hacking-extortion-election-interference-the-toolkit-of-israels-agents-of-chaos/00000186-4aa6-d933-af9e-cbe7aa9c0000) sur les réseaux sociaux sous forme de posts, d'images et de vidéos.

Bientôt, elle sera utilisée par les hackers pour trouver des failles de sécurité dans n'importe quel système informatique.


## :x Fiability

En 2016, le chatbot "Tay" de Microsoft est devenu [incontrôlable](https://www.francetvinfo.fr/replay-radio/nouveau-monde/tay-lintelligence-artificielle-qui-deraille-a-cause-de-la-betise-humaine_1779265.html) moins de 24 après son lancement.

Depuis, les systèmes d'IA continuent de tromper les humains, de manière de plus en plus subtile. Ils [inventent des faits](https://fr.wikipedia.org/wiki/Hallucination_(intelligence_artificielle)) et [cachent des informations stratégiquement](https://www.youtube.com/watch?v=5f7KiFcvsgE).


## :x Pause

Nous ne pouvons pas nous permettre de compter uniquement sur le bon vouloir d'entreprises comme OpenAI, Anthropic ou Mistral. Les enjeux sont bien trop grands, ils ont un pouvoir de même calibre que la bombe atomique. 

https://pauseai.info/

