---
title:
  - Le fonctionnement de chatgpt
---

> Comment fonctionne ChatGPT ?

Cette question, de plus en plus de personnes se la posent. √Ä la fois des jeunes chercheurs, des ing√©nieurs, mais aussi le grand public. Cette page est une tentative d'expliquer ce que je comprends de cette fameuse IA.

Selon vos connaissances en informatique et le niveau de d√©tail que vous cherchez, la r√©ponse que vous attendez est compl√®tement diff√©rente.

C'est pour cette raison que je vais d√©tailler le fonctionnement de chatGPT en plusieurs parties: l'**id√©e**, l'**historique**, le **m√©canisme**, l'**entrainement** et les **myst√®res**. Chaque partie a des pr√©requis diff√©rents, n'h√©sitez pas √† sauter les parties qui ne vous int√©ressent pas.


# L'id√©e

*pr√©requis*: aucun


Partons du fonctionnement de votre cerveau. Vous √™tes en pleine discussion, et une phrase se forme dans votre cerveau. Vous √™tes fatigu√©, si bien que les mots qui sortent de votre bouche se forment de mani√®re presque automatique. Vous pouvez m√™me faire l'experience d√®s √† pr√©sent: partez d'un mot, puis formez une phrase en ajoutant des mots un par un, ceci de plus en plus vite.



Que fait votre cerveau √† ce moment ? Comment est-ce possible que votre cerveau forme des phrases relativement coh√©rentes de mani√®re presque automatique ?

La r√©ponse probable √† cette question, c'est que votre cerveau utilise les derniers mots de la phrase pour trouver le prochain mot le plus probable.

> Ce soir, je vais chercher les ...

Imm√©diatement, votre cerveau liste les mots possibles, et les classe du plus au moins probable. *Enfants* ? *Pizza* ? *Photos* ? *deux* ?

√Ä noter que dans une discussion, le cerveau prend en compte la phrase pr√©c√©dente pour trouver le mot adapt√©.


L'id√©e derri√®re chatGPT, ce n'est pas beaucoup plus compliqu√© que √ßa.

1) √âtape 1: cr√©er un programme qui est tr√®s bon √† trouver le mot suivant dans une phrase
2) √âtape 2: Partir d'un texte donn√©, et trouver le mot suivant le plus probable
3) √âtape 3: ajouter ce mot √† la phrase
4) R√©p√©ter les √©tapes 2 et 3 jusqu'√† obtenir le texte de la longueur donn√©e

On obtient alors un robot qui est tr√®s bon √† inventer la suite d'une phrase. On parle de LLM-fondation (pour fondation de **Large Mod√®le de Langage**). Ce n'est pas chatGPT, mais on s'en approche. On peut d'ailleurs teste l'exemple donn√© plus haut avec le mod√®le fondation de openAI, l'entreprise derri√®re chatGPT:

![[Pasted image 20240829215020.png]]


Le probl√®me, c'est qu'obtenir un robot capable de pr√©dire avec pr√©cision le mot suivant dans une phrase est extr√™mement difficile. Les chercheurs se sont cass√© les dents pendant des d√©cennies sur ce probl√®me, jusqu'√† une innovation majeure en 2018 (que je d√©taillerais dans la suite). On a eu GPT1, GPT2, GPT3, et maintenant GPT4. √Ä chaque fois, les chercheurs de chez openAI (l'entreprise derri√®re chatGPT) ont fait avaler des quantit√©s toujours plus √©normes de texte pour pr√©dire quel est le prochain mot dans des contextes de plus en plus vari√©s.

 Ce que les cr√©ateurs de openAI ont d√©couvert, c'est qu'une fois ce robot construit, il suffit de "l'√©duquer" pour obtenir un mod√®le utile. Des gens sont pay√©s pour lancer le mod√®le, voir la r√©ponse, et lui donner une note. Il y a ensuite une phase d'**entrainement** suppl√©mentaire pour que les r√©ponses soient √† la fois coh√©rentes et probables.


**Pour aller plus loin**

Si vous n'√™tes pas s√ªr de comprendre comment cet phase d'√©ducation fonctionne, vous pouvez aller voir [cette vid√©o](https://www.youtube.com/watch?v=qV_rOlHjvvs)

# L'Historique

*pr√©requis*: savoir ce qu'est le machine learning

Pendant longtemps, les chercheurs en IA ont cherch√© √† faire des syst√®mes qui comprennent le langage avec des algorithmes *symboliques*: on d√©coupe la phrase selon une grammaire, on raisonne dessus, on essaie de trouver l'intention du locuteur. Et on applique des r√®gles bien souvent √©crites √† la main.

Fabriquer ce syst√®me √©tait un genre d'artisanat. Mais les chercheur se sont rendu compte encore et encore que le langage √©tait extr√™mement difficile √† ma√Ætriser: on est tous capable de comprendre une langue √† force de pratique, mais on a aucune id√©e de comment notre cerveau le fait.^[Les linguistes et les neurologues font d'ailleurs des th√®ses pour essayer de comprendre des subtilit√©s de certaines parties du langage, c'est donc illusoire de faire un algorithme qui comprend le langage √† la main]

Jusqu'en 2017, on a fait quelques progr√®s pour traiter le langage humain avec de l'apprentissage supervis√©. Cela demandait beaucoup de travail humain pour indiquer  manuellement les propri√©t√©s de phrase: intention, th√®me, traduction ... Mais les progr√®s √©taient tr√®s lents.

En 2018, openAI utilise une innovation faite par Google l'ann√©e pr√©c√©dente: l'**auto-attention par produit scalaire**. Leur mod√®le est GPT1, et c'est une r√©volution.

GPT signifie "Generative Pre-trained Transformer". Leur approche, c'est entertainer un mod√®le √† pr√©dire le prochain mot d'une phrase (Un mod√®le **g√©n√©ratif** et **pr√©-entrain√©**), puis d'incorporer cette base dans un mod√®le plus gros (une esp√®ce de chirurgie) pour l‚Äôentra√Æner √† un autre objectif. Par exemple: classifier des texte ou r√©pondre √† des questions √† choix multiples. Les r√©sultats sont bien meilleurs que toute la recherche pr√©c√©dente.

Les ann√©es suivantes, openAI emploie des ing√©nieurs pour augmenter la taille du mod√®le et l‚Äôentra√Æner sur toujours plus de donn√©es. GPT2 voit le jour, puis GPT3, GPT4 et toutes ses variantes.

Mais surtout, ils en font un chatbot pour que les utilisateurs puissent l'utiliser, et ils l‚Äôentra√Ænent sp√©cifiquement √† r√©pondre de mani√®re intelligente √† l'utilisateur.


**Pour aller plus loin**

Vous pouvez aller lire le papier de recherche initial de openAI: https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf


## Le m√©canisme

*pr√©requis*: savoir comment fonctionne un r√©seau de neurones et id√©alement l'**embedding** type word2vec.

Pour comprendre le m√©canisme d'un GPT, je vous conseille vivement ces deux vid√©os de la cha√Æne 3b1b:
- [Qu'est-ce qu'un GPT](https://www.youtube.com/watch?v=wjZofJX0v4M)
- [L'attention, au coeur des transformers](https://www.youtube.com/watch?v=eMlx5fFNoYc&t=269s)

Ma d√©marche va √™tre l√©g√®rement diff√©rentes, mais les id√©es sont les m√™mes.

## Situation hypoth√©tique

> Cet exemple illustre ce qu'est l'auto-attention par produit scalaire.

$n$ personnes sont dans un village. Chaque jour, ces personnes interagissent entre elles. Et chaque jour, leur √©tat mental va √©voluer.

On repr√©sente l'√©tat de chaque personne par un vecteur de grande dimension (disons $d=42$). Vous pouvez imaginer que chaque composante correspond √† une mesure d'un trait mental de la personne (Sa fatigue, sa curiosit√©, sa motivation, ou m√™me √† quel point il aime les chats ...). Si vous n'√™tes pas √† l'aise avec les embeddings, dites vous seulement qu'il est possible d‚Äôadditionner des vecteurs (C'est √† dire de cumuler des √©tats), et de faire des produits scalaires (savoir si deux √©tats sont proches ou non).

Votre but: construire un mod√®le qui pr√©dit l'humeur de chaque personne le jour suivant.

Prenons un exemple. On a $n=2$ personnes:
- Alice, qui est fatigu√©e et √©nerv√©e
- Bob, qui est en pleine forme mais se sent blagueur

On peut s'attendre √† ce que le jour suivant Bob fasse une blague, mais que Alice le prenne tr√®s mal. De son c√¥t√©, Alice peut regretter ses paroles et √™tre en col√®re envers elle m√™me.

Comment mod√©liser ceci avec les outils du machine learning ?

Le mod√®le que vous pouvez utiliser, c'est une **couche d'auto-attention par produit scalaire**.

On a besoin de:
- un vecteur pour chaque personne $p_1, p_2 \dots p_n$ de dimension $d=42$
- Une transformation lin√©aire $Q$ qui associe √† chaque $(p_i)$ une **query** $(q_i)$. Ce vecteur a moins de dimensions, disons $m=5$
- Une transformation lin√©aire $K$ qui associe √† chaque $(p_i)$ une **cl√©** $(k_i)$ de dimension $m=5$
- Une transformation lin√©aire $(V)$ qui associe √† chaque $(p_i)$ une **valeur** $(v_i)$ de dimension $d=42$


L'id√©e, c'est que $q_i$ contient les informations sur √† quel point la personne $p_i$ s‚Äô√©nerve facilement, et $c_i$ contient les informations sur √† quel point la personne $p_i$ √©nerve facilement les gens. En calculant le produit scalaire $q_i \cdot k_j$, on sait √† quel point la personne $p_j$ va √©nerver la personne $p_i$. Point important: ici, on consid√®re qu'une personne peut s'√©nerver elle m√™me.

Enfin, le vecteur $v_i$ indique la modification de l'humeur de $p_i$ si on l‚Äô√©nerve.

On peut maintenant calculer le nouvel √©tat $p'$ des individus. Commen√ßons avec $n=2$.

$$
\begin{align}
p'_0 = p_0 + (k_0 \cdot q_0) v_0 + (k_1 \cdot q_0) v_1 \\
p'_1 = p_1 + (k_0 \cdot q_1) v_0 + (k_1 \cdot q_1) v_1 \\
\end{align}
$$

On peut g√©n√©raliser pour $n$ quelconque √† l'aide de matrices:

$$
P' = P+QK^TV
$$


> [!warning] Softmax
> Les deux √©quations pr√©c√©dentes sont fausses: il faut en fait calculer un softmax avant de multiplier par $V$. Je vous renvoie √† la vid√©o de 3b1b.


Illustrons les √©tapes de calcul:

1) on calcule les queries et les cl√©s avec une transformation matricielle:

![[gpt-explained 2024-08-30 23.28.00.excalidraw.svg]]
%%[[gpt-explained 2024-08-30 23.28.00.excalidraw.md|üñã Edit in Excalidraw]]%%

2) on calcule les produits scalaires

![[gpt-explained 2024-08-30 18.09.23.excalidraw.svg]]
%%[[gpt-explained 2024-08-30 18.09.23.excalidraw.md|üñã Edit in Excalidraw]]%%

3) on utilise un softmax pour obtenir une distribution de somme totale √©gale √† 1

![[gpt-explained 2024-08-30 23.26.42.excalidraw.svg]]
%%[[gpt-explained 2024-08-30 23.26.42.excalidraw.md|üñã Edit in Excalidraw]]%%

4) On calcule les $v_i$

![[gpt-explained 2024-08-30 23.33.49.excalidraw.svg]]
%%[[gpt-explained 2024-08-30 23.33.49.excalidraw.md|üñã Edit in Excalidraw]]%%

5) On utilise le tableau de valeurs pr√©c√©dent pour pond√©rer les vecteurs $v_i$, et on somme chaque colonne avec le vecteur $p_i$ initial

![[gpt-explained 2024-08-30 23.35.03.excalidraw.svg]]
%%[[gpt-explained 2024-08-30 23.35.03.excalidraw.md|üñã Edit in Excalidraw]]%%


On va appeler ce qu'on vient de construire une **t√™te d'attention de l‚Äô√©nervement**.

Pour am√©liorer notre mod√®le, on peut ajouter deux choses importantes.

D'abord, on peut ajouter d'autres t√™tes d'attention. On recopie tout le processus pr√©c√©dent avec des matrices $Q$, $C$ et $V$ diff√©rentes. Par exemple, une t√™te pour l'amour, une t√™te pour la tristesse ... √Ä vous d'√™tre imaginatif ! Il suffit alors de sommer les contributions de chaque t√™te d'attention pour avoir le r√©sultat final. On parle d'**attention √† t√™tes multiples**.

Ensuite, on peut ajouter un **masque d'attention**. Si la personne $p_a$ n‚Äôinterag√Æt pas avec la personne $p_b$ au cours de cette journ√©e, il faut le prendre en compte dans le calcul. Pour cela, rien de plus simple: on remplace les valeurs par $-\infty$ avant d'appliquer le softmax dans le motif d'attention aux cases correspondantes.

Vous savez maintenant tout de l'innovation qui a permis l'apparition des LLM. Il ne reste plus qu'√† l'appliquer au langage.

## Adapter l'id√©e pour le texte

L'architecture pr√©c√©dente peut ne pas para√Ætre tr√®s utile pour du texte. Et pourtant, avec quelques adaptations, on peut obtenir un mod√®le tr√®s performant.

Notons $m_0, \dots, m_{n-1}$ les mots d'un texte.

La premi√®re √©tape, c'est de faire un *embedding*: on associe √† chaque mot de la langue fran√ßaise un vecteur de dimension $d$. Cette association est apprise par le mod√®le pendant l'entrainement.

Ensuite, il faut encoder la position de chaque mot. En effet, la couche d'attention est invariante par permutation des vecteurs en entr√©e. Cela signifie que le mod√®le ne peut pas faire la diff√©rence entre "Le chat mange la souris" et "la souris mange le chat". La solution, c'est d'ajouter √† chaque vecteur un vecteur $e(pos)$ avec $pos$ l'indexe du mot dans la phrase. 

Pour cela, plusieurs solutions sont possibles. Historiquement, la solution propos√©e par le papier qui a introduit le m√©canisme d'attention est celle ci:
$$
e(pos)_j = \begin{cases}
\sin(\dfrac{pos}{10000^{j/d}}) \text{ si $j$ est pair} \\[1em]
\cos(\dfrac{pos}{10000^{(j-1)/d}}) \text{ si $j$ est impair}
\end{cases}
$$

C'est √† dire que chaque composante du vecteur est une fonction trigonom√©trique de la position avec une fr√©quence diff√©rente.

Mais tous les mod√®les GPT utilisent une id√©e diff√©rente: un vecteur de position appris. De la m√™me mani√®re que "chien" correspond √† un vecteur de dimension $d$, le mot en position 42 correspond √† un autre vecteur de dimension $d$. Il suffit d'ajouter les deux pour obtenir un vecteur signifiant que le mot "chien" est en position 42.

Le dernier changement √† effectuer correspond √† l'intuition qu'un mot √† la fin de la phrase ne devrait pas modifier la signification d'un mot au d√©but de la phrase. Or, la couche d'attention va modifier l'interpr√©tation de chaque mot en utilisant tous les autres. 
Pour √©viter ce probl√®me, on utilise un **masque d'attention** causal qui va emp√™cher une cl√© $c_i$ d'interagir avec une query $q_j$ d'un mot pr√©c√©dent. Le masque d'attention ressemble √† ceci:

![[gpt-explained 2024-08-31 12.28.10.excalidraw.svg]]
%%[[gpt-explained 2024-08-31 12.28.10.excalidraw.md|üñã Edit in Excalidraw]]%%

Ce masque d'attention permet une autre propri√©t√© int√©ressante: en sortie de notre bloc d'attention, on obtient en colonne $i$ l'embedding du mot pr√©dit √† partir de **l'ensemble des mots jusqu'√† la position i**. Cette propri√©t√© permet d'am√©liorer l'efficacit√© de l'entrainement (je d√©taille l'entrainement plus bas).

Pour effectuer notre pr√©diction, il ne reste plus que 2 √©tapes:
- Appliquer un unembedding au vecteur $p'_{n-1}$, c'est √† dire appliquer une transformation lin√©aire au dernier vecteur pour obtenir un score pour chaque mot (la matrice d'embedding est apprise lors de l'entrainement).
- Appliquer un softmax pour obtenir une distribution de probabilit√©

![[gpt-explained 2024-08-31 15.53.47.excalidraw.svg]]
%%[[gpt-explained 2024-08-31 15.53.47.excalidraw.md|üñã Edit in Excalidraw]]%%


Si on impl√©mente et qu'on entra√Æne ce mod√®le, on va pouvoir pr√©dire des mots.

Mais la diversit√© des motifs que l'on peut apprendre est assez limit√©e: on ne peut apprendre que des r√®gles sur des couples de mots, par nature du m√©canisme d'attention. Par exemple, on peut apprendre que si le premier mot est "le" est qu'un mot en position $(n-1)$ est "est", alors le mot suivant sera probablement "un".
Cela reste tr√®s limit√©: si on veut identifier des motifs plus compliqu√©s, il faut ajouter des couches.
## L'architecture GPT

On a enfin tous les ingr√©dients n√©cessaires pour construire GPT.

Dans la partie pr√©c√©dente, j'ai fait une simplification: le texte est d√©coup√© en mots. En r√©alit√©, il est d√©coup√© en tokens: des mots, des parties de mots, des symboles ou des rep√®res sp√©ciaux (fin de texte par exemple).

Avant m√™me de construire le mod√®le, il faut donc choisir un **vocabulaire**, c'est √† dire un ensemble de tokens. On note $M$ la taille du vocabulaire 

Il faut ensuite d√©couper notre texte en les mots de ce vocabulaire. C'est ce qu'on appelle la tokenization, et ce n'est pas tr√®s compliqu√©. Vous pouvez jouer avec [ici](https://gpt-tokenizer.dev/)

![[Pasted image 20240830235728.png]]

On obtient une s√©quence de num√©ros, les num√©ros correspondants √† notre vocabulaire. Notons les $t_0, \cdots t_{k-1}$

Ensuite, on les transforme en vecteurs de taille $M$. Si le premier token est le $j$-eme token de notre vocabulaire, on place un 1 en $j$-eme position et des 0 partout ailleurs. On note $T_0, \dots T_{k-1}$ 


> [!NOTE] Context window
> En pratique, la taille de l'entr√©e que l'on donne au mod√®le est toujours la m√™me. On ajoute donc un token sp√©cial de padding √† la fin du texte si celui-ci est plus court que la "context window", et on modifie le masque d'attention pour dire au mod√®le de ne pas prendre en compte ces tokens dans les calculs.


Cette s√©quence $T_0 \dots T_{k-1}$ est l'entr√©e de notre GPT.

Voici l'architecture:

![[gpt-explained 2024-08-31 16.12.41.excalidraw.svg]]
%%[[gpt-explained 2024-08-31 16.12.41.excalidraw.md|üñã Edit in Excalidraw]]%%

Quelques pr√©cisions:
- La taille de l'entr√©e et la taille de la sortie est exactement la m√™me: $k$ vecteurs de dimension $M$ (la taille du vocabulaire), et la somme d'un vecteur vaut 1 en entr√©e et en sortie. Toutes les couches interm√©diaires sont elles compos√©es de $k$ vecteurs de dimension $d$
- Le bloc rouge s'appelle un "transformer bloc". Il est constitu√© d'une couche d'attention multi-t√™te suivi d'un r√©seau de neurone classique de deux couches. Ces couches op√®rent vecteur par vecteur. Dans la litt√©rature, on les appelle des "feed-forward network" et elles sont d√©finies de cette fa√ßon: $FFN(x) = \sigma(xW1 + b1)W2 + b2$
- La matrice obtenue sur la derni√®re couche donne en fait $k$ pr√©dictions diff√©rentes: $P_i$ donne donne la distribution de probabilit√© pour le token en position $i$ √©tant donn√© tous les tokens de positions inf√©rieure √† $i$. Pour effectuer une pr√©diction, on s'int√©resse donc uniquement √† $P_{k-1}$

**pour aller plus loin**

Un [article technique](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbTZEVl9fYjNqM1RPdzIzOHQxS0NjSklDNkN3QXxBQ3Jtc0tuV294VGp6d1pFNl9FNGcwUzhOYUVMWS1hRmlpYndDYkJSbUxVa2JFbkhIaER6eG1Bb18zNlZwNE9zRFVTbUtid0pza0I0QmZmdlJUZG1CSm1ub1ZrckRtRzlBd2NyRjlSenpXT3dfS25fWGtJc2ZJYw&q=https%3A%2F%2Ftransformer-circuits.pub%2F2021%2Fframework%2Findex.html&v=eMlx5fFNoYc) de Anthropic qui prouve diff√©rents r√©sultats sur les GPT.

# Entrainement

#todo


# Myst√®res
