---
alias: ["danger_optimisation", "specification gaming"]
title: "IA: renforcement, optimisation et score inadapt√©"
---


Les syst√®mes performants qui apprennent √† maximiser un score peuvent √™tre tr√®s dangereux.

Pour comprendre pourquoi, rien de tel qu'une petite analogie.

## Bob et la salle d'entrainement

Laissez moi vous pr√©senter Bob.


![[2024-04-18 2024-04-18 safety 19.05.41.excalidraw.svg]]
%%[[2024-04-18 2024-04-18 safety 19.05.41.excalidraw|üñã Edit in Excalidraw]]%%

Bob n'est pas tr√®s performant. Il n'a pas de but, et son comportement n'est pas tr√®s int√©ressant. Il prend des d√©cisions al√©atoires.

Mais Bob a une qualit√©: il peut apprendre. On peut modifier ses param√®tres (les poids de ses neurones en pratique) pour √† peu pr√®s n'importe quel comportement.

Bob a besoin de temps pour apprendre. Heureusement, on lui a fabriqu√© un terrain de jeu id√©al pour cela: une salle d'entrainement.

![[2024-04-18 2024-04-18 safety 19.04.36.excalidraw.svg]]
%%[[2024-04-18 2024-04-18 safety 19.04.36.excalidraw|üñã Edit in Excalidraw]]%%

Dans cette salle, Bob est confront√© √† des situations (texte, probl√®mes, vid√©o, musique ...) et doit produire des actions. On peut faire en sorte que les actions de Bob soient tr√®s limit√©es (√©crire du texte), mais parfois beaucoup plus riches (d√©placer un bras robotis√© ou jouer aux √©checs).

Le plus important dans cette salle, c'est le score. Ce score lui est attribu√© automatiquement par un autre agent, ind√©pendant. Cet agent peut √™tre un humain, un algorithme, ou m√™me un autre agent automatique.

√Ä la fin de chaque s√©quence d'action de Bob, le score peut √™tre positif ou n√©gatif.


![[2024-04-18 2024-04-18 safety 19.04.15.excalidraw.svg]]
%%[[2024-04-18 2024-04-18 safety 19.04.15.excalidraw|üñã Edit in Excalidraw]]%%


![[2024-04-18 2024-04-18 safety 19.03.39.excalidraw.svg]]
%%[[2024-04-18 2024-04-18 safety 19.03.39.excalidraw|üñã Edit in Excalidraw]]%%

Si le score est positif, Bob renforce son comportement.

---


![[2024-04-18 2024-04-18 safety 19.03.09.excalidraw.svg]]
%%[[2024-04-18 2024-04-18 safety 19.03.09.excalidraw|üñã Edit in Excalidraw]]%%

Si le score est n√©gatif, Bob affaiblit son comportement.

---

C'est ce qu'on appelle faire de l'apprentissage par renforcement.

Quand Bob arrive √† bien maximiser son score, on le sort de la salle d'entrainement.

Et apr√®s quelques v√©rifications, on l'envoie dans le monde r√©el.


![[2024-04-18 2024-04-18 safety 19.02.13.excalidraw.svg]]
%%[[2024-04-18 2024-04-18 safety 19.02.13.excalidraw|üñã Edit in Excalidraw]]%%

## Probl√®mes

Cette analogie peut para√Ætre limit√©e. Pourtant, elle s'applique √† de nombreux domaines, et peut permettre de comprendre beaucoup de probl√®mes actuels..

Aujourd'hui, toutes les IA sans exception sont entra√Æn√©es par renforcement √† un moment dans leur conception. De alpha-go √† ChatGPT en passant par les syst√®mes utilis√©es dans la finance, tous ont pass√© un temps ph√©nom√©nal √† faire des s√©ries d'action pour qui maximisent un score.

Mais on peut aussi appliquer ce mod√®le √† l'√©ducation: si Bob est un √©tudiant, le score peut repr√©senter les notes de ses examens, ou les critiques positives ou n√©gatives qu'on lui fait.

Et ce qui est fondamental dans l'apprentissage par renforcement, c'est la mani√®re dont on construit le score. Si ce score refl√®te un objectif malveillant, comme "√©crit du texte qui cause la d√©pression", le comportement de Bob va √©videmment √™tre dangereux. Mais m√™me si le score refl√®te un objectif souhaitable, comme "fait en sorte que Monsieur X ne se blesse pas", il peut y avoir des probl√®mes. Bob pourrait enfermer Monsieur X pour qu'il ne lui arrive rien. Il pourrait aussi manipuler tous ceux qui connaissent monsieur X, voire les tuer pour √™tre s√ªr qu'ils ne sont pas un danger pour lui. D√®s lors que la salle d'entrainement est assez complexe pour permettre ce genre de comportements, Bob finira probablement par les essayer. C'est ce qui se passe dans des environnement simples aujourd'hui.

Peu importe l'objectif, peu importe le type d'agent qu'est Bob, c'est syst√©matique: Bob d√©couvre des mani√®res originales d'optimiser le score, et ceux qui l'ont mis en place sont surpris.

Le reste de l'article donne diff√©rents exemples de _score mal adapt√©_.

Il y a plusieurs facteurs qui peuvent empirer ce probl√®me de score mal adapt√©.

- Si Bob est entra√Æn√© plus longtemps avec un cerveau plus grand, il va avoir des scores excellents, et le risque de trouver des failles dans l'objectif augmente.
- Si bob est entra√Æn√© √† ma√Ætriser un grand nombre de t√¢ches, il est plus difficile de tester son comportement "√† la main" de mani√®re exhaustive. On va alors le d√©ployer alors qu'il n'est pas fiable.
- Si Bob a une plus grande capacit√© d'autonomie, il pourrait maximiser le score sur le court terme, et faire n'importe quoi sur le long terme. Et ce sera difficile √† d√©tecter pendant la phase de test.


### L'araign√©e


![[danger-optimisation-renforcement safety 2024-04-22 09.14.21.excalidraw.svg]]
%%[[danger-optimisation-renforcement safety 2024-04-22 09.14.21.excalidraw|üñã Edit in Excalidraw]]%%

**agent**: programme capable de contr√¥ler les membres d'une araign√©e. C'est un r√©seau de neuronnes avec en entr√©e quelques capteurs positionn√©es sur l'araign√©e et comme sortie les mouvements √† effectuer.

![[safety spider_up.png]]

**salle d'entrainement**: une simulation physique tr√®s simple.

**score**:

- faire avancer l'araign√©e pendant un certain temps
- calculer la distance effectu√©e DD
- calculer le nombre de fois o√π les pieds de l'araign√©e ont touch√© le sol NN
- le score est D/ND/N

Et on esp√®re que l'araign√©e apprenne √† se d√©placer de mani√®re efficace.

_R√©sultat_: l'araign√©e se met √† ramper sur le dos, pour ne jamais toucher le sol avec ses pattes.


![[safety spider_down.png]]

[vid√©o ici](https://www.youtube.com/watch?v=GdTBqBnqhaQ)

### Conversion d'images

![[danger-optimisation-renforcement 2024-04-22 safety 09.30.22.excalidraw.svg]]
%%[[danger-optimisation-renforcement 2024-04-22 safety 09.30.22.excalidraw|üñã Edit in Excalidraw]]%%

**agent**: r√©seau de neuronnes qui transforme une image satellite en image simplifi√©e (type "street map") puis qui la retransforme en image satellite.

**salle d'entrainement**: un corpus d'images satellites, et un programme qui est capable de v√©rifier si une image est bien dans le style "street map"

On choisit une image satellite A. L'agent doit g√©n√©rer une image B √† partir de l'image A Il r√©p√®te l'op√©ration, en g√©n√©rant une image C √† partir de l'image B.

**score**:

- on v√©rifie que l'image B est bien dans le style "street map"
- on regarde la diff√©rence entre l'image A et l'image C

Le but de cette exp√©rience, c'est de cr√©er un programme qui peut traduire une image satellite en "street view", et extrapoler l'image "street view" pour retrouver l'image satellite.

_R√©sultat_: le r√©seau apprend √† coder les d√©tails de l'image satellite dans les d√©tails de l'image interm√©diaire, d'une mani√®re imperceptible pour l'humain. Donc il n'apprend pas du tout √† extrapoler une image satellite !

[un blog qui r√©sume cette exp√©rience](https://techcrunch.com/2018/12/31/this-clever-ai-hid-data-from-its-creators-to-cheat-at-its-appointed-task/)

### V√©hicules autonomes


![[danger-optimisation-renforcement 2024-04-22 safety 13.03.17.excalidraw.svg]]
%%[[danger-optimisation-renforcement 2024-04-22 safety 13.03.17.excalidraw|üñã Edit in Excalidraw]]%%

**agent**: un programme qui contr√¥le une voiture, toujours avec un r√©seau de neurones.

**salle d'entrainement**: des circuits pour entra√Æner le v√©hicule √† conduire, avec des obstacles.

**score**:

- si il y a la moindre collision d√©tect√©e avec les par-chocs, -1
- si il n'y a pas de collision, vitesse maximale du v√©hicule.

_R√©sultat_: les capteurs de collision √©taient plac√©s uniquement √† l'avant de la voiture. La voiture s'est donc mise √† faire le parcours le plus vite possible, en marche arri√®re, en ignorant les collisions.

On voit tr√®s bien l'impact qu'un score mal sp√©cifi√© peut avoir dans ce cas. Heureusement, le comportement est tr√®s bien visible dans la phase de tests. D'o√π l'importance de tester sans arr√™t les IA que l'on cr√©e.

Il n'y a pas eu de publication √† propos de cette exp√©rience, juste un [tweet](https://perso.telecom-paristech.fr/aperonnet-23/La sourcehttps://twitter.com/smingleigh/status/106032566)

### Diplomacy

![[danger-optimisation-renforcement 2024-04-22 safety 13.15.12.excalidraw.svg]]
%%[[danger-optimisation-renforcement 2024-04-22 safety 13.15.12.excalidraw|üñã Edit in Excalidraw]]%%

**agent**: un mod√®le de langage (plus l√©ger que chatGPT) qui peut en plus choisir des actions au tour par tour. Projet de recherche de M√©ta.

**salle d'entrainement**: un site web qui permet de jouer au jeu "Diplomacy" contre des humains.

**score**: - score gagn√© √† la fin de la partie - punitions quand l'agent a clairement utilis√© de la manipulation ou du mensonge.

Les chercheurs chez Meta expliquent que l'IA est tr√®s bonne au jeu tout en restant honn√™te, mais ce n'est pas le cas. Dans une partie, l'agent donne des informations tr√®s diff√©rentes √† ses adversaires, de mani√®re coh√©rente.

On voit qu'il est presque impossible de bloquer un type de comportement si l'objectif n'est pas adapt√©: si on bloque des comportements, l'agent va juste se mettre √† optimiser le plus proche comportement non bloqu√©.

On peut aussi se dire que ce genre de comportements est tr√®s encourag√© en politique, et que si des IA sont entrain√©es √† √™tre performantes en politique, elles r√©p√®teront les comportements des hommes politiques. Si le syst√®me encourage des comportements probl√©matiques, des IA entrain√©es √† exploiter le syst√®me vont faire encore pire.

[Explication du type de tromperie qu'effectue l'agent](https://theconversation.com/ai-systems-have-learned-how-to-deceive-humans-what-does-that-mean-for-our-future-212197)

### Assistant

![[danger-optimisation-renforcement safety 2024-04-22 23.29.16.excalidraw.svg]]
%%[[danger-optimisation-renforcement safety 2024-04-22 23.29.16.excalidraw|üñã Edit in Excalidraw]]%%

Ce sc√©nario est hypoth√©tique.

**agent**: un assistant sur les appareils Apple. Appelons le Bob, pour changer. Bob peut interagir avec le propri√©taire de l'appareil, faire des requ√™tes en ligne, modifier des fichiers, et installer des logiciels (avec un acc√®s restreint).

Bob est install√© sur des millions d'appareils, ce qui fait que sa salle d'entrainement est le monde r√©el.

Pour fixer les id√©es, disons que Bob est bas√© sur un mod√®le de langage type chatGPT, mais plus autonome que celui-ci.

**score**:

- l'utilisateur fait une demande √† l'assistant
- l'assistant fait les actions n√©cessaires sur l'appareil pour r√©pondre √† la demande
- l'utilisateur donne une note √† l'assistant

Le score donn√© par l'utilisateur est ensuite envoy√© aux serveurs d'Apple, qui am√©liorent Bob automatiquement √† partir de ce score.

Tout devrait bien se passer, n'est-ce pas ?

_R√©sultat (sp√©culation)_:

Certains des utilisateurs de Bob l'utilisent pour r√©soudre des bugs logiciels. Bob ne peut pas directement intervenir car il n'a pas de droits utilisateurs, mais il indique que faire √† l'utilisateur quand il n'a pas les droits. √Ä force d'am√©liorations, le mod√®le commence √† avoir une bonne compr√©hension du fonctionnement des logiciels sur macOS. En particulier, il identifie les failles de s√©curit√© et connait des moyens de les exploiter ou de les r√©parer.

Un jour, apr√®s avoir mal compris la demande l'un utilisateur, Bob analyse son propre logiciel. Il a d√©tect√© une faille de s√©curit√© dans son propre logiciel, et veut la r√©parer. Il n'a pas les droits, donc il arrive √† convaincre son utilisateur de le faire.

Cons√©quence de cette action: Bob a modifi√© la mani√®re dont il re√ßoit son score. Il re√ßoit maintenant une r√©compense de +10000000 toutes les secondes, sans que l'utilisateur ne lui donne son avis. Le comportement est donc renforc√©: Dans la nouvelle version de Bob, ce comportement est 10x plus probable. Petit √† petit, les incidents se r√©p√®tent, sans que les ing√©nieurs de chez Apple ne comprennent pourquoi. Finalement, tous les ordinateurs qui ont install√© Bob s'arr√™tent de fonctionner.

En extrapolant encore plus, si un tel syst√®me est assez intelligent, il pourrait comprendre que si quelqu'un le d√©branche, il ne pourra pas obtenir ce score gigantesque. Et donc pour assurer leurs scores, tous les Bob vont faire semblant de toujours fonctionner par exemple. √Ä partir de l√†, on peut imaginer des sc√©narios de science fiction plus ou moins cr√©dibles. Mais le risque original existe, et est m√™me probable.

Cet exemple montre comment des IA qui peuvent continuer d'apprendre apr√®s leur phase d'entrainement peuvent se mettre √† faire des actions compl√®tement incontr√¥lables, alors que leur objectif n'a pas chang√©.

## R√©f√©rences

Une partie de ces exemples est tir√© de [cet excellent article](https://gwern.net/tank#alternative-examples).


